{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1.Conception of AI and DL.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMy9+7cf/lW4j4pHkPME69s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# 딥러닝 / 인공지능의 이해"],"metadata":{"id":"2HOh05JmI1f3"}},{"cell_type":"markdown","source":["### AI(Mimic Human Intelligence) > MachineLearning(Data-driven Approach) > DeepLearning(Deep Neural Networks)"],"metadata":{"id":"wBgbTv5yJBxI"}},{"cell_type":"markdown","source":["### 일반적인 프로그래밍 (규칙 기반 {Rule Driven})\n","- input 과 program(규칙) 을 컴퓨터에 주고 output을 뽑는다"],"metadata":{"id":"vbp0kZ-aJWBm"}},{"cell_type":"markdown","source":["### 기계학습 (데이터 기반 {Data Driven})\n","- input , output 을 Computer 에게 보여주고 기계가 program을 직접 작성"],"metadata":{"id":"4AL9SOFdKjGf"}},{"cell_type":"markdown","source":["### Machine Learnig 의 종류\n","- Supervised Learning(지도 학습) \n","    - 입력 data 와 정답을 이용한 학습 \n","    - 분류 (classification) , 회귀 (regression)\n","- Unsupervised Learning(비지도 학습) \n","    - 입력 data 만을 이용하여 학습\n","    - 군집화(clustering) , 압축(compression)\n","- Reinforcement Learning(강화 학습)\n","    - Trial and error 를 통한 학습\n","    - Action selection, Policy learning"],"metadata":{"id":"J2NRdtAKKqp2"}},{"cell_type":"markdown","source":["### Training and Testing\n"],"metadata":{"id":"oJKWp3k8LFyP"}},{"cell_type":"markdown","source":["### Training Stage : \n","- Input Data => Learning System(Model) => Correct Output\n"],"metadata":{"id":"xu02G3w8LoFv"}},{"cell_type":"markdown","source":["### Testing Stage : (머신러닝의 궁극적인 목적 : 새로운 데이터에 대한 예측) \n","- New Input Data => Learned System(Model) => Best Guess"],"metadata":{"id":"yNVpFc68LxM4"}},{"cell_type":"markdown","source":["# DATA"],"metadata":{"id":"FKm-vTi2L6Be"}},{"cell_type":"markdown","source":["- Training set, Validation set, Test set\n","- Training => Tuning(validation) => Test \n","- In any case  , Do Not Use the test set before the training is over !!!!! "],"metadata":{"id":"K3bfjNZ3MlGm"}},{"cell_type":"markdown","source":["- Data is Important \n","- Data is expensive"],"metadata":{"id":"bGEjCHCCM4zj"}},{"cell_type":"markdown","source":["### Good Data vs Bad Data\n","- Our data is unbiased ??? \n","- 한 쪽으로 치우친 데이터가 아닌지 의심\n","- Our labels are perfect ?? \n","- Garbage In Garbage Out\n"],"metadata":{"id":"E3bbYbZ7NwxG"}},{"cell_type":"markdown","source":["### Data - Centric AI\n","- AI System = Code + Data\n"],"metadata":{"id":"uacZZPjaONUt"}},{"cell_type":"markdown","source":["### Model-Centric AI \n","- How can you change the model (code) to improve performance ? "],"metadata":{"id":"j3HPypmcOXzV"}},{"cell_type":"markdown","source":["### Data-Centric AI \n","- How can you systematically change the data (inputs x or labels y) to improve performance ? \n","- The following are about equally effective :    \n","    - Clean up the noise \n","    - Collect another many new examples\n","- With a data centric view, there is a significant of room for improvement in problems with < 10,000 examples"],"metadata":{"id":"Fyun_GDeOgq2"}},{"cell_type":"markdown","source":["# Artificial Neural Network\n","- weight 값을 기계가 스스로 학습을 통해 찾아내도록 하는 것이 neural network 를 이용한 기계학습이 하는 일"],"metadata":{"id":"pMsyMTBoPJhr"}},{"cell_type":"markdown","source":["- weights : 연결 강도를 표현"],"metadata":{"id":"5azwVpRVPJlI"}},{"cell_type":"markdown","source":["### Importance of Activation Functions\n","- The purpose of activation functions is to introduce non-linearities into the network \n","- Linear Activation functions produce linear decisions no matter the network size \n","- Non-linearities allow us to approximate arbitrarily complex functions"],"metadata":{"id":"nYWyUlrhPJn0"}},{"cell_type":"markdown","source":["### Single-Layer Percetron & Multi-Layer Perceptron(MLP) \n","- Simgle-Layer Perceptron (Input - Output) \n","- MLP (Input - hidden - Output)"],"metadata":{"id":"GLPnopTLPJqo"}},{"cell_type":"markdown","source":["### Deep Learning \n","- Deep Neural Network 을 이용한 Machine Learning 방법 \n","- Deep Neural Network : Hidden layer 수가 최소 2개 이상인 network"],"metadata":{"id":"OKN1GYbbPJtP"}},{"cell_type":"markdown","source":["# Training Neural Networks \n","- 최적의 weight 를 어떻게 찾을까 ? \n","- 아무것도 모르는 상태에서 시작하는데 weight 의 초기값은 어떻게? : Random initialization (Normal Distribution)\n","- 잘 모르겟으니 일단 아무 값이나 넣고 시작한다. "],"metadata":{"id":"NIApzexxPJv1"}},{"cell_type":"markdown","source":["### Loss Function / Cost Function \n","- Neural Network 이 얼마나 잘하는 지 또는 얼마나 못하는 지에 대한 척도 \n","- Loss function 의 값이 줄어들도록 weight 값들을 조금씩 바꾸는 것 \n","    - Weight 를 어떻게 바꿔야 Loss function 값이 줄어들까 ? ==> 미분\n","    - Loss 를 w 로 미분하고, 미분값이 가리키는 방향의 반대방향으로 아주 조금씩 w 를 바꿔나가면 Loss 를 감소시킬 수 있다. "],"metadata":{"id":"2HbkxI9oSAh3"}},{"cell_type":"markdown","source":["### Gradient Descent (경사 하강법)\n","- Loss Function 의 미분(Gradient) 를 이용하여 weight 를 updata 하는 방법\n","- Weight updata = W*new *- W*old*\n","- - (Loss 를 감소 시키는 방향 {Descent}) * Learning Rate(아주 조금씩 이동) * 미분값 (Gradient) "],"metadata":{"id":"8bU2po_0SAkN"}},{"cell_type":"markdown","source":["### Back Propagation \n","- Loss 로부터 거꾸로 한 단계식 미분 값을 구하고 이 값들을 chain rule 에 의하여 곱해가면서 weight 에 대한 gradient 를 구하는 방법"],"metadata":{"id":"fKKLQy5TSAm8"}},{"cell_type":"markdown","source":["# Key Components of Deep Learning\n","- The Data that the model can learn from \n","- The Model how to transform the data \n","- The loss function that quantifies the badness of the model \n","- The algorithm to adjust the parameters to minimize the loss"],"metadata":{"id":"kCyaGE0ASApl"}},{"cell_type":"markdown","source":["# Historical Review of Deep Learning"],"metadata":{"id":"p38uNeZ8SAsn"}},{"cell_type":"markdown","source":["### 2012 - Alex Net\n"],"metadata":{"id":"EJhpWrcfSAvb"}},{"cell_type":"markdown","source":["### 2013 - Playing Atari with Deep Reinforcement Learning"],"metadata":{"id":"oJ8FdJWZSAyM"}},{"cell_type":"markdown","source":["### 2014 - Encoder-Decoder Network with Attention (번역 모델의 성장)"],"metadata":{"id":"BzGZYqSCSA00"}},{"cell_type":"markdown","source":["### 2014 - Adam Optimizer (only find learning rate)"],"metadata":{"id":"E5atSMZ_SA3r"}},{"cell_type":"markdown","source":["### 2014 / 2015 - Generative Adversarial Networks (GAN)"],"metadata":{"id":"PVr7qmKbPJyu"}},{"cell_type":"markdown","source":["### 2015 - Residual Networks (ResNet) - 152 layers .. "],"metadata":{"id":"a88cjXQJPJ4N"}},{"cell_type":"markdown","source":["### 2016 - AlphaGo"],"metadata":{"id":"qvYUZdj3PJ6c"}},{"cell_type":"markdown","source":["### 2017 - Transformer (Attention is All You Need) _ Google (번역) _ 자연어 정복 _ 이미지 분야에도 점점 적용"],"metadata":{"id":"UThX8cAMPJ94"}},{"cell_type":"markdown","source":["### 2018 - BERT and Fine - tuned NLP Models"],"metadata":{"id":"Pu5r-iE4WIKk"}},{"cell_type":"markdown","source":["### 2019/2020 - BIG Language Models"],"metadata":{"id":"Kso2c5bhWXmb"}},{"cell_type":"markdown","source":["### 2018/2020 - Self-Supervised Learning"],"metadata":{"id":"9SswXH9zWtC3"}},{"cell_type":"code","source":[""],"metadata":{"id":"VcX0UEuDWyCa"},"execution_count":null,"outputs":[]}]}